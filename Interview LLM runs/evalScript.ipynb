{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON\n",
    "with open('apiResponse/all_responses_200_sample.json', 'r', encoding='utf-8') as file:\n",
    "    api_data = json.load(file)\n",
    "\n",
    "with open('Samples/sample_siirtokarjalaiset_annotated.json', 'r', encoding='utf-8') as file:\n",
    "    hand_data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 items. Cleaned data saved back to the file.\n"
     ]
    }
   ],
   "source": [
    "def clean_api_response_data(file_path, items_to_remove):\n",
    "    # Convert all items in the list to lowercase for comparison\n",
    "    items_to_remove = [item.lower().strip() for item in items_to_remove]\n",
    "    \n",
    "    # Load the json data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    removed_count = 0\n",
    "    \n",
    "    for entry in data:\n",
    "        api_response = entry.get(\"api_response\", \"\")\n",
    "        api_items = api_response.split(\"\\n\")\n",
    "        \n",
    "        cleaned_api_items = []\n",
    "        for item in api_items:\n",
    "            if \": \" not in item:  # Check if \": \" exists in the line\n",
    "                continue  # If not, skip to the next iteration\n",
    "\n",
    "            # Split into key and values\n",
    "            key, value = item.split(\": \", 1)\n",
    "            \n",
    "            # Split values based on comma, while stripping any extra spaces\n",
    "            values_list = [v.strip() for v in value.split(\",\")]\n",
    "            cleaned_values_list = [v for v in values_list if v.lower() not in items_to_remove and v.strip() != '']\n",
    "                \n",
    "            # Update count of removed items\n",
    "            removed_count += len(values_list) - len(cleaned_values_list)\n",
    "                \n",
    "            # Recombine to a single string\n",
    "            cleaned_value = \", \".join(cleaned_values_list)\n",
    "            cleaned_api_items.append(f\"{key}: {cleaned_value}\")\n",
    "                \n",
    "        # Recombine the cleaned API items to the api_response format\n",
    "        entry[\"api_response\"] = \"\\n\".join(cleaned_api_items)\n",
    "    \n",
    "    # Save the cleaned data back to the file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    return f\"Removed {removed_count} items. Cleaned data saved back to the file.\"\n",
    "\n",
    "# Example usage\n",
    "file_path = 'apiResponse/all_responses_200_sample.json'\n",
    "items_to_remove = [\"none\", \"N/A\", \"whateverelse\", \"-\", \" \", \"  \"]\n",
    "print(clean_api_response_data(file_path, items_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FuxxyWUzzy algorithm\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def parse_response(response_str):\n",
    "    lines = response_str.split('\\n')\n",
    "    parsed_response = {}\n",
    "    for line in lines:\n",
    "        key, _, value = line.partition(': ')\n",
    "        parsed_response[key] = value.strip() if value.strip() else None\n",
    "    return parsed_response\n",
    "\n",
    "\n",
    "# Checking how similar the words are\n",
    "def are_similar(str1, str2, threshold=70, context=None):\n",
    "    similarity = fuzz.token_set_ratio(str1, str2)\n",
    "    is_similar = similarity > threshold\n",
    "    \n",
    "    # Check if similarity is below 100% and store it if so\n",
    "    if is_similar and similarity < 100:\n",
    "        store_not_exact_matches(str1, str2, similarity, context)\n",
    "        \n",
    "    return is_similar\n",
    "\n",
    "\n",
    "def store_not_exact_matches(str1, str2, similarity, context):\n",
    "    data = {\n",
    "        \"string_1\": str1,\n",
    "        \"string_2\": str2,\n",
    "        \"similarity\": similarity,\n",
    "        \"context\": context  # This will provide additional information\n",
    "    }\n",
    "    with open(\"non_exact_matches.json\", \"a\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        file.write(\",\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Matches: 390\n",
      "Total Mismatches: 461\n",
      "Match Percentage: 45.83%\n"
     ]
    }
   ],
   "source": [
    "## to pass context to non_exact_matches use this script!\n",
    "\n",
    "def compare_values(api_values, annotated_values, context):\n",
    "    api_list = api_values.lower().split(', ') if api_values else []\n",
    "    annotated_list = annotated_values.lower().split(', ') if annotated_values else []\n",
    "    \n",
    "    matches = set()\n",
    "    mismatches = set(api_list).union(set(annotated_list))  \n",
    "    \n",
    "    for api_val in api_list:\n",
    "        for ann_val in annotated_list:\n",
    "            if are_similar(api_val, ann_val, context=context):\n",
    "                matches.add(api_val)\n",
    "                mismatches.discard(api_val)\n",
    "                mismatches.discard(ann_val)\n",
    "    \n",
    "    return matches, mismatches\n",
    "\n",
    "\n",
    "# Parse JSON strings\n",
    "api_responses = api_data\n",
    "hand_annotated = hand_data\n",
    "# Loop over all elements in api_responses and hand_annotated to compare them\n",
    "results = []\n",
    "total_matches = 0\n",
    "total_mismatches = 0\n",
    "\n",
    "for api_resp in api_responses:\n",
    "    hand_ann = next((item for item in hand_annotated if item[\"index\"] == api_resp[\"person_index\"]), None)\n",
    "    if not hand_ann:\n",
    "        continue\n",
    "    \n",
    "    parsed_api_response = parse_response(api_resp['api_response'])\n",
    "    \n",
    "    comparison_results = {\n",
    "        \"index\": hand_ann['index'],\n",
    "        \"person_name\": hand_ann['primary_person_name'],\n",
    "        \"spouse_name\": hand_ann['spouse_name'],\n",
    "        \"detail\": []\n",
    "    }\n",
    "    \n",
    "    for key in [\"person_hobbies\", \"person_social_orgs\", \"spouse_hobbies\", \"spouse_social_orgs\"]:\n",
    "        split_keys = key.split(\"_\")\n",
    "        api_key = split_keys[0].capitalize() + \"\".join(word.capitalize() for word in split_keys[1:])\n",
    "        \n",
    "        # Safely get the index values for context\n",
    "        api_person_index = api_resp.get('person_index', None)\n",
    "        annotated_person_index = hand_ann.get('index', None)\n",
    "        \n",
    "\n",
    "        # Build the context\n",
    "        context = {\n",
    "            \"api_person_index\": api_person_index,\n",
    "            \"annotated_person_index\": annotated_person_index,\n",
    "            \"category_type\": key\n",
    "        }\n",
    "        \n",
    "        matches, mismatches = compare_values(parsed_api_response.get(api_key, \"\"), hand_ann[key], context)\n",
    "        \n",
    "        detail = {\n",
    "            \"type\": key,\n",
    "            \"matches\": list(matches),\n",
    "            \"mismatches\": list(mismatches)\n",
    "        }\n",
    "        comparison_results[\"detail\"].append(detail)\n",
    "        \n",
    "        total_matches += len(matches)\n",
    "        total_mismatches += len(mismatches)\n",
    "    \n",
    "    results.append(comparison_results)\n",
    "\n",
    "output_json = json.dumps(results, indent=4, ensure_ascii=False)\n",
    "\n",
    "# To store the results in a file:\n",
    "with open(\"matches_results.json\", \"w\") as file:\n",
    "    file.write(output_json)\n",
    "\n",
    "# Printing total matches and mismatches\n",
    "print(f\"Total Matches: {total_matches}\")\n",
    "print(f\"Total Mismatches: {total_mismatches}\")\n",
    "\n",
    "# Calculating and printing the match percentage\n",
    "total_comparisons = total_matches + total_mismatches\n",
    "if total_comparisons > 0: \n",
    "    match_percentage = (total_matches / total_comparisons) * 100\n",
    "    print(f\"Match Percentage: {match_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"No comparisons were made (Total Comparisons: 0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precision, recall and F Score \n",
    "\n",
    "# to pass context to non_exact_matches use the previous script\n",
    "\n",
    "def compare_values(api_dict, hand_ann_dict):\n",
    "    def safe_extract(api_response, keyword):\n",
    "        try:\n",
    "            return api_response.split(keyword)[1].split(\"\\n\")[0].strip().split(', ')\n",
    "        except IndexError:\n",
    "            return []\n",
    "\n",
    "    api_name = safe_extract(api_dict[\"api_response\"], \"PersonName:\")\n",
    "    api_hobbies = safe_extract(api_dict[\"api_response\"], \"PersonHobbies:\")\n",
    "    api_social_orgs = safe_extract(api_dict[\"api_response\"], \"PersonSocialOrgs:\")\n",
    "    api_spouse_hobbies = safe_extract(api_dict[\"api_response\"], \"SpouseHobbies:\")\n",
    "    api_spouse_social_orgs = safe_extract(api_dict[\"api_response\"], \"SpouseSocialOrgs:\")\n",
    "\n",
    "    hand_name = hand_ann_dict[\"primary_person_name\"]\n",
    "    hand_hobbies = hand_ann_dict[\"person_hobbies\"].split(', ')\n",
    "    hand_social_orgs = hand_ann_dict[\"person_social_orgs\"].split(', ')\n",
    "    hand_spouse_hobbies = hand_ann_dict[\"spouse_hobbies\"].split(', ')\n",
    "    hand_spouse_social_orgs = hand_ann_dict[\"spouse_social_orgs\"].split(', ')\n",
    "\n",
    "    # Assuming are_similar function is predefined\n",
    "\n",
    "    def calculate_metrics(api_values, hand_values):\n",
    "        TP = set()\n",
    "        FP = set(api_values)  # Temporarily assume all api_values are FP\n",
    "        FN = set(hand_values)  # Temporarily assume all annotated_values are FN\n",
    "        \n",
    "        for api_val in api_values:\n",
    "            for ann_val in hand_values:\n",
    "                if are_similar(api_val, ann_val):\n",
    "                    TP.add(api_val)  # Add to True Positives\n",
    "                    FP.discard(api_val)  # Remove from False Positives\n",
    "                    FN.discard(ann_val)  # Remove from False Negatives\n",
    "                    break  # Stop looking for additional matches\n",
    "                    \n",
    "        precision = len(TP) / (len(TP) + len(FP)) if TP or FP else 0\n",
    "        recall = len(TP) / (len(TP) + len(FN)) if TP or FN else 0\n",
    "        f_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "        return TP, FP, FN, precision, recall, f_score\n",
    "\n",
    "    # Compute metrics for person and spouse separately\n",
    "    person_metrics = calculate_metrics(api_social_orgs, hand_social_orgs)\n",
    "    person_hobbies = calculate_metrics(api_hobbies, hand_hobbies)\n",
    "    spouse_metrics = calculate_metrics(api_spouse_social_orgs, hand_spouse_social_orgs)\n",
    "    spouse_hobbies = calculate_metrics(api_spouse_hobbies, hand_spouse_hobbies)\n",
    "\n",
    "    \n",
    "\n",
    "    return person_metrics, spouse_metrics, person_hobbies, spouse_hobbies\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "api_responses = api_data  # This should be your API data\n",
    "hand_annotated = hand_data  # This should be your hand annotated data\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "total_person_TP = total_person_FP = total_person_FN = 0\n",
    "total_spouse_TP = total_spouse_FP = total_spouse_FN = 0\n",
    "total_person_hobbies_TP = total_person_hobbies_FP = total_person_hobbies_FN = 0\n",
    "total_spouse_hobbies_TP = total_spouse_hobbies_FP = total_spouse_hobbies_FN = 0\n",
    "\n",
    "\n",
    "for api_resp in api_responses:\n",
    "    hand_ann = next((item for item in hand_annotated if item[\"index\"] == api_resp[\"person_index\"]), None)\n",
    "    if not hand_ann:\n",
    "        continue\n",
    "    \n",
    "    # Call compare_values and unpack all four return values here:\n",
    "    person_metrics, spouse_metrics, person_hobbies_metrics, spouse_hobbies_metrics = compare_values(api_resp, hand_ann)\n",
    "\n",
    "    total_person_TP += len(person_metrics[0])\n",
    "    total_person_FP += len(person_metrics[1])\n",
    "    total_person_FN += len(person_metrics[2])\n",
    "\n",
    "    total_spouse_TP += len(spouse_metrics[0])\n",
    "    total_spouse_FP += len(spouse_metrics[1])\n",
    "    total_spouse_FN += len(spouse_metrics[2])\n",
    "    \n",
    "    # Add handling for person_hobbies_metrics and spouse_hobbies_metrics\n",
    "    # For example:\n",
    "    total_person_hobbies_TP += len(person_hobbies_metrics[0])\n",
    "    total_person_hobbies_FP += len(person_hobbies_metrics[1])\n",
    "    total_person_hobbies_FN += len(person_hobbies_metrics[2])\n",
    "\n",
    "    total_spouse_hobbies_TP += len(spouse_hobbies_metrics[0])\n",
    "    total_spouse_hobbies_FP += len(spouse_hobbies_metrics[1])\n",
    "    total_spouse_hobbies_FN += len(spouse_hobbies_metrics[2])\n",
    "\n",
    "    results.append({\n",
    "    \"person_metrics\": {\n",
    "        \"index\": hand_ann['index'],\n",
    "        \"person_name\": hand_ann['primary_person_name'],\n",
    "        \"true_positives\": list(person_metrics[0]),\n",
    "        \"false_positives\": list(person_metrics[1]),\n",
    "        \"false_negatives\": list(person_metrics[2]),\n",
    "        \"precision\": person_metrics[3],\n",
    "        \"recall\": person_metrics[4],\n",
    "        \"f_score\": person_metrics[5]\n",
    "    },\n",
    "    \"person_hobbies_metrics\": {\n",
    "        \"true_positives\": list(person_hobbies_metrics[0]),\n",
    "        \"false_positives\": list(person_hobbies_metrics[1]),\n",
    "        \"false_negatives\": list(person_hobbies_metrics[2]),\n",
    "        \"precision\": person_hobbies_metrics[3],\n",
    "        \"recall\": person_hobbies_metrics[4],\n",
    "        \"f_score\": person_hobbies_metrics[5]\n",
    "    },\n",
    "    \"spouse_metrics\": {\n",
    "        \"true_positives\": list(spouse_metrics[0]),\n",
    "        \"false_positives\": list(spouse_metrics[1]),\n",
    "        \"false_negatives\": list(spouse_metrics[2]),\n",
    "        \"precision\": spouse_metrics[3],\n",
    "        \"recall\": spouse_metrics[4],\n",
    "        \"f_score\": spouse_metrics[5]\n",
    "    },\n",
    "    \"spouse_hobbies_metrics\": {\n",
    "        \"true_positives\": list(spouse_hobbies_metrics[0]),\n",
    "        \"false_positives\": list(spouse_hobbies_metrics[1]),\n",
    "        \"false_negatives\": list(spouse_hobbies_metrics[2]),\n",
    "        \"precision\": spouse_hobbies_metrics[3],\n",
    "        \"recall\": spouse_hobbies_metrics[4],\n",
    "        \"f_score\": spouse_hobbies_metrics[5]\n",
    "    }\n",
    "})\n",
    "\n",
    "summary = {\n",
    "\n",
    "}\n",
    "for key in summary:\n",
    "    summary[key]['precision'] = round(summary[key]['precision'], 3)\n",
    "    summary[key]['recall'] = round(summary[key]['recall'], 3)\n",
    "    summary[key]['f_score'] = round(summary[key]['f_score'], 3)\n",
    "\n",
    "\n",
    "# Final JSON output to include both the detailed results and the summary\n",
    "output_data = {\n",
    "    \"summary\": summary,\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "# Storing in JSON format\n",
    "output_json = json.dumps(output_data, indent=4, ensure_ascii=False)\n",
    "\n",
    "# To store the results in a file:\n",
    "with open(\"precision_recall_results.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 429 items. Cleaned data saved back to the file.\n"
     ]
    }
   ],
   "source": [
    "#remove words like \"none\", \"N/A\" etc.. \n",
    "def clean_json_data(file_path, items_to_remove):\n",
    "    # Convert all items in the list to lowercase for comparison\n",
    "    items_to_remove = [item.lower() for item in items_to_remove]\n",
    "    \n",
    "    # Load the json data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    removed_count = 0\n",
    "\n",
    "    # Iterate through the \"results\" list and clean unwanted items\n",
    "    for result in data.get(\"results\", []):\n",
    "        # For each metric type (e.g., person_metrics, spouse_metrics)...\n",
    "        for metric_key, metric_values in result.items():\n",
    "            # For each list value (e.g., true_positives, false_positives)...\n",
    "            for list_key, list_values in metric_values.items():\n",
    "                if isinstance(list_values, list):\n",
    "                    # Filter out unwanted items and empty strings or strings with only spaces\n",
    "                    cleaned_list = [value for value in list_values if value.lower() not in items_to_remove and value.strip() != '']\n",
    "                    \n",
    "                    # Update count of removed items\n",
    "                    removed_count += len(list_values) - len(cleaned_list)\n",
    "\n",
    "                    # Update the list in the JSON data\n",
    "                    metric_values[list_key] = cleaned_list\n",
    "\n",
    "    # Save the cleaned data back to the file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    return f\"Removed {removed_count} items. Cleaned data saved back to the file.\"\n",
    "\n",
    "# Example usage\n",
    "file_path = 'precision_recall_results.json'\n",
    "items_to_remove = [\"none\", \"N/A\", \"-\", \" \", \"  \"]\n",
    "print(clean_json_data(file_path, items_to_remove))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#micro f score\n",
    "#summary for all categories\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(TP, FP, FN):\n",
    "    try:\n",
    "        precision = round((TP / (TP + FP)) if TP + FP != 0 else 0, 2)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "        \n",
    "    try:\n",
    "        recall = round((TP / (TP + FN)) if TP + FN != 0 else 0, 2)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "        \n",
    "    try:\n",
    "        f_score = round((2 * precision * recall) / (precision + recall) if precision + recall != 0 else 0, 2)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0.0\n",
    "    \n",
    "    return precision, recall, f_score\n",
    "\n",
    "# Load the JSON data\n",
    "with open('precision_recall_results.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize metrics counters\n",
    "metrics_counter = {\n",
    "    \"person\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"spouse\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"social_orgs\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"hobbies\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"person_social_orgs\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"person_hobbies\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"spouse_social_orgs\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"spouse_hobbies\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"overall\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"overall_no_emptylabel\": {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
    "}\n",
    "\n",
    "# Iterate through the \"results\" section\n",
    "# Helper function to update the metrics\n",
    "def update_metrics(category_key, value):\n",
    "    metrics_counter[category_key][\"TP\"] += len(value[\"true_positives\"])\n",
    "    metrics_counter[category_key][\"FP\"] += len(value[\"false_positives\"])\n",
    "    metrics_counter[category_key][\"FN\"] += len(value[\"false_negatives\"])\n",
    "\n",
    "    # Update the \"overall_no_emptylabel\" category\n",
    "    if category_key == \"overall\":\n",
    "        metrics_counter[\"overall_no_emptylabel\"][\"TP\"] += len([tp for tp in value[\"true_positives\"] if tp != \"emptylabel\"])\n",
    "        metrics_counter[\"overall_no_emptylabel\"][\"FP\"] += len([fp for fp in value[\"false_positives\"] if fp != \"emptylabel\"])\n",
    "        metrics_counter[\"overall_no_emptylabel\"][\"FN\"] += len([fn for fn in value[\"false_negatives\"] if fn != \"emptylabel\"])\n",
    "\n",
    "\n",
    "# Iterate through the \"results\" section\n",
    "for result in data['results']:\n",
    "    for key, value in result.items():\n",
    "        if \"person_metrics\" in key:\n",
    "            update_metrics(\"person_social_orgs\", value)\n",
    "            update_metrics(\"person\", value)\n",
    "            update_metrics(\"social_orgs\", value)\n",
    "\n",
    "        if \"person_hobbies_metrics\" in key:\n",
    "            update_metrics(\"person_hobbies\", value)\n",
    "            update_metrics(\"person\", value)\n",
    "            update_metrics(\"hobbies\", value)\n",
    "\n",
    "        if \"spouse_metrics\" in key:\n",
    "            update_metrics(\"spouse_social_orgs\", value)\n",
    "            update_metrics(\"spouse\", value)\n",
    "            update_metrics(\"social_orgs\", value)\n",
    "\n",
    "        if \"spouse_hobbies_metrics\" in key:\n",
    "            update_metrics(\"spouse_hobbies\", value)\n",
    "            update_metrics(\"spouse\", value)\n",
    "            update_metrics(\"hobbies\", value)\n",
    "\n",
    "        # Update overall metrics\n",
    "        update_metrics(\"overall\", value)\n",
    "\n",
    "# Update summary metrics in data\n",
    "for category, metrics in metrics_counter.items():\n",
    "    precision, recall, f_score = calculate_metrics(metrics[\"TP\"], metrics[\"FP\"], metrics[\"FN\"])\n",
    "    data[\"summary\"][category] = {\n",
    "        \"total_true_positives\": metrics[\"TP\"],\n",
    "        \"total_false_positives\": metrics[\"FP\"],\n",
    "        \"total_false_negatives\": metrics[\"FN\"],\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f_score\": f_score\n",
    "    }\n",
    "\n",
    "# Save the updated data back to the JSON file\n",
    "with open('precision_recall_results.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average Precision: 0.26\n",
      "Macro-average Recall: 0.25\n",
      "Macro-average F-score: 0.25\n",
      "--\n",
      "Micro-average Precision: 0.76\n",
      "Micro-average Recall: 0.53\n",
      "Micro-average F-score: 0.62\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#macro f score\n",
    "# summary for all categories\n",
    "\n",
    "\n",
    "# Load the JSON data\n",
    "with open('precision_recall_results.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "averages = []\n",
    "\n",
    "for result in data[\"results\"]:\n",
    "    precision_sum = 0.0\n",
    "    recall_sum = 0.0\n",
    "    f_score_sum = 0.0\n",
    "    \n",
    "    entities = [\"person_metrics\", \"person_hobbies_metrics\", \"spouse_metrics\", \"spouse_hobbies_metrics\"]\n",
    "    num_entities = len(entities)\n",
    "    \n",
    "    for entity in entities:\n",
    "        precision_sum += result[entity][\"precision\"]\n",
    "        recall_sum += result[entity][\"recall\"]\n",
    "        f_score_sum += result[entity][\"f_score\"]\n",
    "    \n",
    "    avg_precision = precision_sum / num_entities\n",
    "    avg_recall = recall_sum / num_entities\n",
    "    avg_f_score = f_score_sum / num_entities\n",
    "    \n",
    "    averages.append({\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f_score\": avg_f_score\n",
    "    })\n",
    "\n",
    "# Now `averages` contains the average precision, recall, and F-score\n",
    "# for each person in the original \"results\" array.\n",
    "\n",
    "total_precision = 0.0\n",
    "total_recall = 0.0\n",
    "total_f_score = 0.0\n",
    "num_instances = len(averages)\n",
    "\n",
    "# Summing up all the average scores\n",
    "for avg in averages:\n",
    "    total_precision += avg['average_precision']\n",
    "    total_recall += avg['average_recall']\n",
    "    total_f_score += avg['average_f_score']\n",
    "\n",
    "# Calculating the macro-average for all instances\n",
    "macro_avg_precision = total_precision / num_instances\n",
    "macro_avg_recall = total_recall / num_instances\n",
    "macro_avg_f_score = total_f_score / num_instances\n",
    "\n",
    "micro_avg_precision = round((metrics_counter[\"overall\"][\"TP\"] / (metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FP\"])) if metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FP\"] != 0 else 0, 2)\n",
    "micro_avg_recall = round((metrics_counter[\"overall\"][\"TP\"] / (metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FN\"])) if metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FN\"] != 0 else 0, 2)\n",
    "micro_avg_f_score = round((2 * micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall) if micro_avg_precision + micro_avg_recall != 0 else 0, 2)\n",
    "\n",
    "data[\"summary\"][\"micro_average\"] = {\n",
    "    \"total_true_positives\": metrics_counter[\"overall\"][\"TP\"],\n",
    "    \"total_false_positives\": metrics_counter[\"overall\"][\"FP\"],\n",
    "    \"total_false_negatives\": metrics_counter[\"overall\"][\"FN\"],\n",
    "    \"precision\": micro_avg_precision,\n",
    "    \"recall\": micro_avg_recall,\n",
    "    \"f_score\": micro_avg_f_score\n",
    "}\n",
    "\n",
    "# Add macro averages to the summary\n",
    "data[\"summary\"][\"macro_average\"] = {\n",
    "    \"total_true_positives\": None,  # These values are not defined in macro-averaging\n",
    "    \"total_false_positives\": None,  # These values are not defined in macro-averaging\n",
    "    \"total_false_negatives\": None,  # These values are not defined in macro-averaging\n",
    "    \"precision\": macro_avg_precision,\n",
    "    \"recall\": macro_avg_recall,\n",
    "    \"f_score\": macro_avg_f_score\n",
    "}\n",
    "\n",
    "# Save the updated data back to the JSON file\n",
    "with open('precision_recall_results.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print Macro and Micro averages\n",
    "print(f\"Macro-average Precision: {macro_avg_precision:.2f}\")\n",
    "print(f\"Macro-average Recall: {macro_avg_recall:.2f}\")\n",
    "print(f\"Macro-average F-score: {macro_avg_f_score:.2f}\")\n",
    "print(\"--\")\n",
    "print(f\"Micro-average Precision: {data['summary']['micro_average']['precision']:.2f}\")\n",
    "print(f\"Micro-average Recall: {data['summary']['micro_average']['recall']:.2f}\")\n",
    "print(f\"Micro-average F-score: {data['summary']['micro_average']['f_score']:.2f}\")\n",
    "print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 'empty label' 411 times.\n"
     ]
    }
   ],
   "source": [
    "## Evaluation for true negative # EMPTY LABEL for updated results\n",
    "## Basicly adding emptylabel in true positive if there is no false positive or false negative\n",
    "\n",
    "# Load the JSON data\n",
    "with open('precision_recall_results.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "empty_label_assigned_count = 0  # Counter for empty labels assigned\n",
    "\n",
    "# Iterate through the \"results\" section\n",
    "for result in data['results']:\n",
    "    for metric_key, metric_value in result.items():\n",
    "        \n",
    "        # Check if all entities are empty\n",
    "        if not any(metric_value['true_positives']) and \\\n",
    "           not any(metric_value['false_positives']) and \\\n",
    "           not any(metric_value['false_negatives']):\n",
    "            metric_value['true_positives'] = [\"emptylabel\"]\n",
    "            metric_value['f_score'] = 1.0\n",
    "            metric_value['precision'] = 1.0\n",
    "            metric_value['recall'] = 1.0\n",
    "            empty_label_assigned_count += 1  # Increment the counter\n",
    "        else:\n",
    "            # Calculate precision, recall, and F-score\n",
    "            try:\n",
    "                metric_value['precision'] = len(metric_value['true_positives']) / \\\n",
    "                                            (len(metric_value['true_positives']) + len(metric_value['false_positives']))\n",
    "            except ZeroDivisionError:\n",
    "                metric_value['precision'] = 0.0\n",
    "                \n",
    "            try:\n",
    "                metric_value['recall'] = len(metric_value['true_positives']) / \\\n",
    "                                         (len(metric_value['true_positives']) + len(metric_value['false_negatives']))\n",
    "            except ZeroDivisionError:\n",
    "                metric_value['recall'] = 0.0\n",
    "                \n",
    "            try:\n",
    "                metric_value['f_score'] = 2 * (metric_value['precision'] * metric_value['recall']) / \\\n",
    "                                          (metric_value['precision'] + metric_value['recall'])\n",
    "            except ZeroDivisionError:\n",
    "                metric_value['f_score'] = 0.0\n",
    "\n",
    "# Save the updated data back to the JSON file\n",
    "with open('precision_recall_updated_results.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Print how many times the \"empty label\" was assigned\n",
    "print(f\"Assigned 'empty label' {empty_label_assigned_count} times.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 items. Cleaned data saved back to the file.\n"
     ]
    }
   ],
   "source": [
    "#remove words like \"none\", \"N/A\" etc.. \n",
    "def clean_json_data(file_path, items_to_remove):\n",
    "    # Convert all items in the list to lowercase for comparison\n",
    "    items_to_remove = [item.lower() for item in items_to_remove]\n",
    "    \n",
    "    # Load the json data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    removed_count = 0\n",
    "\n",
    "    # Iterate through the \"results\" list and clean unwanted items\n",
    "    for result in data.get(\"results\", []):\n",
    "        # For each metric type (e.g., person_metrics, spouse_metrics)...\n",
    "        for metric_key, metric_values in result.items():\n",
    "            # For each list value (e.g., true_positives, false_positives)...\n",
    "            for list_key, list_values in metric_values.items():\n",
    "                if isinstance(list_values, list):\n",
    "                    # Filter out unwanted items and empty strings or strings with only spaces\n",
    "                    cleaned_list = [value for value in list_values if value.lower() not in items_to_remove and value.strip() != '']\n",
    "                    \n",
    "                    # Update count of removed items\n",
    "                    removed_count += len(list_values) - len(cleaned_list)\n",
    "\n",
    "                    # Update the list in the JSON data\n",
    "                    metric_values[list_key] = cleaned_list\n",
    "\n",
    "    # Save the cleaned data back to the file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    return f\"Removed {removed_count} items. Cleaned data saved back to the file.\"\n",
    "\n",
    "# Example usage\n",
    "file_path = 'precision_recall_updated_results.json'\n",
    "items_to_remove = [\"none\", \"N/A\", \"-\", \" \", \"  \"]\n",
    "print(clean_json_data(file_path, items_to_remove))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#micro f score # EMPTY LABEL\n",
    "#summary for all categories\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(TP, FP, FN):\n",
    "    try:\n",
    "        precision = round((TP / (TP + FP)) if TP + FP != 0 else 0, 2)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "        \n",
    "    try:\n",
    "        recall = round((TP / (TP + FN)) if TP + FN != 0 else 0, 2)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "        \n",
    "    try:\n",
    "        f_score = round((2 * precision * recall) / (precision + recall) if precision + recall != 0 else 0, 2)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0.0\n",
    "    \n",
    "    return precision, recall, f_score\n",
    "\n",
    "# Load the JSON data\n",
    "with open('precision_recall_updated_results.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize metrics counters\n",
    "metrics_counter = {\n",
    "    \"person\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"spouse\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"social_orgs\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"hobbies\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"person_social_orgs\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"person_hobbies\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"spouse_social_orgs\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"spouse_hobbies\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "    \"overall\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n",
    "}\n",
    "\n",
    "# Iterate through the \"results\" section\n",
    "# Helper function to update the metrics\n",
    "def update_metrics(category_key, value):\n",
    "    metrics_counter[category_key][\"TP\"] += len(value[\"true_positives\"])\n",
    "    metrics_counter[category_key][\"FP\"] += len(value[\"false_positives\"])\n",
    "    metrics_counter[category_key][\"FN\"] += len(value[\"false_negatives\"])\n",
    "\n",
    "\n",
    "# Iterate through the \"results\" section\n",
    "for result in data['results']:\n",
    "    for key, value in result.items():\n",
    "        if \"person_metrics\" in key:\n",
    "            update_metrics(\"person_social_orgs\", value)\n",
    "            update_metrics(\"person\", value)\n",
    "            update_metrics(\"social_orgs\", value)\n",
    "\n",
    "        if \"person_hobbies_metrics\" in key:\n",
    "            update_metrics(\"person_hobbies\", value)\n",
    "            update_metrics(\"person\", value)\n",
    "            update_metrics(\"hobbies\", value)\n",
    "\n",
    "        if \"spouse_metrics\" in key:\n",
    "            update_metrics(\"spouse_social_orgs\", value)\n",
    "            update_metrics(\"spouse\", value)\n",
    "            update_metrics(\"social_orgs\", value)\n",
    "\n",
    "        if \"spouse_hobbies_metrics\" in key:\n",
    "            update_metrics(\"spouse_hobbies\", value)\n",
    "            update_metrics(\"spouse\", value)\n",
    "            update_metrics(\"hobbies\", value)\n",
    "\n",
    "        # Update overall metrics\n",
    "        update_metrics(\"overall\", value)\n",
    "\n",
    "# Update summary metrics in data\n",
    "for category, metrics in metrics_counter.items():\n",
    "    precision, recall, f_score = calculate_metrics(metrics[\"TP\"], metrics[\"FP\"], metrics[\"FN\"])\n",
    "    data[\"summary\"][category] = {\n",
    "        \"total_true_positives\": metrics[\"TP\"],\n",
    "        \"total_false_positives\": metrics[\"FP\"],\n",
    "        \"total_false_negatives\": metrics[\"FN\"],\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f_score\": f_score\n",
    "    }\n",
    "\n",
    "# Save the updated data back to the JSON file\n",
    "with open('precision_recall_updated_results.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average Precision: 0.77\n",
      "Macro-average Recall: 0.76\n",
      "Macro-average F-score: 0.76\n",
      "--\n",
      "Micro-average Precision: 0.87\n",
      "Micro-average Recall: 0.70\n",
      "Micro-average F-score: 0.78\n",
      "----------\n",
      "Person 1 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 2 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 3 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 4 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 5 - Average Precision: 1.00, Average Recall: 0.88, Average F-score: 0.92\n",
      "Person 6 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 7 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 8 - Average Precision: 0.75, Average Recall: 0.62, Average F-score: 0.67\n",
      "Person 9 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 10 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 11 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 12 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 13 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 14 - Average Precision: 0.62, Average Recall: 0.75, Average F-score: 0.67\n",
      "Person 15 - Average Precision: 0.25, Average Recall: 0.14, Average F-score: 0.18\n",
      "Person 16 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 17 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 18 - Average Precision: 1.00, Average Recall: 0.81, Average F-score: 0.88\n",
      "Person 19 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 20 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 21 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 22 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 23 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 24 - Average Precision: 0.90, Average Recall: 0.90, Average F-score: 0.90\n",
      "Person 25 - Average Precision: 0.25, Average Recall: 0.08, Average F-score: 0.12\n",
      "Person 26 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 27 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 28 - Average Precision: 0.88, Average Recall: 0.88, Average F-score: 0.88\n",
      "Person 29 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 30 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 31 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 32 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 33 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 34 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 35 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 36 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 37 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 38 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 39 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 40 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 41 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 42 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 43 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 44 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 45 - Average Precision: 0.88, Average Recall: 0.83, Average F-score: 0.85\n",
      "Person 46 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 47 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 48 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 49 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 50 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 51 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 52 - Average Precision: 0.62, Average Recall: 0.62, Average F-score: 0.62\n",
      "Person 53 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 54 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 55 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 56 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 57 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 58 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 59 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 60 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 61 - Average Precision: 0.71, Average Recall: 0.45, Average F-score: 0.52\n",
      "Person 62 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 63 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 64 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 65 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 66 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 67 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 68 - Average Precision: 0.92, Average Recall: 0.92, Average F-score: 0.92\n",
      "Person 69 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 70 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 71 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 72 - Average Precision: 0.70, Average Recall: 0.70, Average F-score: 0.70\n",
      "Person 73 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 74 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 75 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 76 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 77 - Average Precision: 0.75, Average Recall: 0.62, Average F-score: 0.67\n",
      "Person 78 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 79 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 80 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 81 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 82 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 83 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 84 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 85 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 86 - Average Precision: 1.00, Average Recall: 0.88, Average F-score: 0.92\n",
      "Person 87 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 88 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 89 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 90 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 91 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 92 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 93 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 94 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 95 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 96 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 97 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 98 - Average Precision: 1.00, Average Recall: 0.88, Average F-score: 0.92\n",
      "Person 99 - Average Precision: 0.83, Average Recall: 0.88, Average F-score: 0.85\n",
      "Person 100 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 101 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 102 - Average Precision: 0.50, Average Recall: 0.38, Average F-score: 0.42\n",
      "Person 103 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 104 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 105 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 106 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 107 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 108 - Average Precision: 0.50, Average Recall: 0.42, Average F-score: 0.45\n",
      "Person 109 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 110 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 111 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 112 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 113 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 114 - Average Precision: 1.00, Average Recall: 0.88, Average F-score: 0.92\n",
      "Person 115 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 116 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 117 - Average Precision: 0.50, Average Recall: 0.42, Average F-score: 0.45\n",
      "Person 118 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 119 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 120 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 121 - Average Precision: 1.00, Average Recall: 0.90, Average F-score: 0.94\n",
      "Person 122 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 123 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 124 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 125 - Average Precision: 0.88, Average Recall: 0.88, Average F-score: 0.88\n",
      "Person 126 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 127 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 128 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 129 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 130 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 131 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 132 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 133 - Average Precision: 0.92, Average Recall: 0.92, Average F-score: 0.92\n",
      "Person 134 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 135 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 136 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 137 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 138 - Average Precision: 1.00, Average Recall: 0.94, Average F-score: 0.96\n",
      "Person 139 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 140 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 141 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 142 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 143 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 144 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 145 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 146 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 147 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 148 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 149 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 150 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 151 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 152 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 153 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 154 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 155 - Average Precision: 0.88, Average Recall: 0.82, Average F-score: 0.84\n",
      "Person 156 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 157 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 158 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 159 - Average Precision: 0.08, Average Recall: 0.25, Average F-score: 0.12\n",
      "Person 160 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 161 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 162 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 163 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 164 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 165 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 166 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 167 - Average Precision: 1.00, Average Recall: 0.92, Average F-score: 0.95\n",
      "Person 168 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 169 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 170 - Average Precision: 0.88, Average Recall: 0.88, Average F-score: 0.88\n",
      "Person 171 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 172 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 173 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 174 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 175 - Average Precision: 0.44, Average Recall: 0.46, Average F-score: 0.43\n",
      "Person 176 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 177 - Average Precision: 0.88, Average Recall: 1.00, Average F-score: 0.92\n",
      "Person 178 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 179 - Average Precision: 1.00, Average Recall: 0.88, Average F-score: 0.92\n",
      "Person 180 - Average Precision: 0.88, Average Recall: 0.88, Average F-score: 0.88\n",
      "Person 181 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 182 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 183 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 184 - Average Precision: 0.13, Average Recall: 0.38, Average F-score: 0.18\n",
      "Person 185 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 186 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 187 - Average Precision: 0.00, Average Recall: 0.00, Average F-score: 0.00\n",
      "Person 188 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 189 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 190 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n",
      "Person 191 - Average Precision: 1.00, Average Recall: 0.88, Average F-score: 0.92\n",
      "Person 192 - Average Precision: 0.88, Average Recall: 0.94, Average F-score: 0.88\n",
      "Person 193 - Average Precision: 0.50, Average Recall: 0.50, Average F-score: 0.50\n",
      "Person 194 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 195 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 196 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 197 - Average Precision: 1.00, Average Recall: 1.00, Average F-score: 1.00\n",
      "Person 198 - Average Precision: 0.25, Average Recall: 0.25, Average F-score: 0.25\n",
      "Person 199 - Average Precision: 0.88, Average Recall: 1.00, Average F-score: 0.92\n",
      "Person 200 - Average Precision: 0.75, Average Recall: 0.75, Average F-score: 0.75\n"
     ]
    }
   ],
   "source": [
    "#macro f score # EMPTY LABEL\n",
    "# summary for all categories\n",
    "\n",
    "\n",
    "# Load the JSON data\n",
    "with open('precision_recall_updated_results.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "averages = []\n",
    "\n",
    "for result in data[\"results\"]:\n",
    "    precision_sum = 0.0\n",
    "    recall_sum = 0.0\n",
    "    f_score_sum = 0.0\n",
    "    \n",
    "    entities = [\"person_metrics\", \"person_hobbies_metrics\", \"spouse_metrics\", \"spouse_hobbies_metrics\"]\n",
    "    num_entities = len(entities)\n",
    "    \n",
    "    for entity in entities:\n",
    "        precision_sum += result[entity][\"precision\"]\n",
    "        recall_sum += result[entity][\"recall\"]\n",
    "        f_score_sum += result[entity][\"f_score\"]\n",
    "    \n",
    "    avg_precision = precision_sum / num_entities\n",
    "    avg_recall = recall_sum / num_entities\n",
    "    avg_f_score = f_score_sum / num_entities\n",
    "    \n",
    "    averages.append({\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f_score\": avg_f_score\n",
    "    })\n",
    "\n",
    "# Now `averages` contains the average precision, recall, and F-score\n",
    "# for each person in the original \"results\" array.\n",
    "\n",
    "total_precision = 0.0\n",
    "total_recall = 0.0\n",
    "total_f_score = 0.0\n",
    "num_instances = len(averages)\n",
    "\n",
    "# Summing up all the average scores\n",
    "for avg in averages:\n",
    "    total_precision += avg['average_precision']\n",
    "    total_recall += avg['average_recall']\n",
    "    total_f_score += avg['average_f_score']\n",
    "\n",
    "# Calculating the macro-average for all instances\n",
    "macro_avg_precision = total_precision / num_instances\n",
    "macro_avg_recall = total_recall / num_instances\n",
    "macro_avg_f_score = total_f_score / num_instances\n",
    "\n",
    "micro_avg_precision = round((metrics_counter[\"overall\"][\"TP\"] / (metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FP\"])) if metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FP\"] != 0 else 0, 2)\n",
    "micro_avg_recall = round((metrics_counter[\"overall\"][\"TP\"] / (metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FN\"])) if metrics_counter[\"overall\"][\"TP\"] + metrics_counter[\"overall\"][\"FN\"] != 0 else 0, 2)\n",
    "micro_avg_f_score = round((2 * micro_avg_precision * micro_avg_recall) / (micro_avg_precision + micro_avg_recall) if micro_avg_precision + micro_avg_recall != 0 else 0, 2)\n",
    "\n",
    "data[\"summary\"][\"micro_average\"] = {\n",
    "    \"total_true_positives\": metrics_counter[\"overall\"][\"TP\"],\n",
    "    \"total_false_positives\": metrics_counter[\"overall\"][\"FP\"],\n",
    "    \"total_false_negatives\": metrics_counter[\"overall\"][\"FN\"],\n",
    "    \"precision\": micro_avg_precision,\n",
    "    \"recall\": micro_avg_recall,\n",
    "    \"f_score\": micro_avg_f_score\n",
    "}\n",
    "\n",
    "# Add macro averages to the summary\n",
    "data[\"summary\"][\"macro_average\"] = {\n",
    "    \"total_true_positives\": None,  # These values are not defined in macro-averaging\n",
    "    \"total_false_positives\": None,  # These values are not defined in macro-averaging\n",
    "    \"total_false_negatives\": None,  # These values are not defined in macro-averaging\n",
    "    \"precision\": macro_avg_precision,\n",
    "    \"recall\": macro_avg_recall,\n",
    "    \"f_score\": macro_avg_f_score\n",
    "}\n",
    "\n",
    "# Save the updated data back to the JSON file\n",
    "with open('precision_recall_updated_results.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print Macro and Micro averages\n",
    "print(f\"Macro-average Precision: {macro_avg_precision:.2f}\")\n",
    "print(f\"Macro-average Recall: {macro_avg_recall:.2f}\")\n",
    "print(f\"Macro-average F-score: {macro_avg_f_score:.2f}\")\n",
    "print(\"--\")\n",
    "print(f\"Micro-average Precision: {data['summary']['micro_average']['precision']:.2f}\")\n",
    "print(f\"Micro-average Recall: {data['summary']['micro_average']['recall']:.2f}\")\n",
    "print(f\"Micro-average F-score: {data['summary']['micro_average']['f_score']:.2f}\")\n",
    "print(\"----------\")\n",
    "for i, avg in enumerate(averages):\n",
    "   print(f\"Person {i+1} - Average Precision: {avg['average_precision']:.2f}, Average Recall: {avg['average_recall']:.2f}, Average F-score: {avg['average_f_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print false_positives and false_negatives\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_false_data(file_path, output_path):\n",
    "    # Load the json data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Dictionaries to collect data\n",
    "    false_positives_dict = {}\n",
    "    false_negatives_dict = {}\n",
    "\n",
    "    for result in data.get(\"results\", []):\n",
    "        index = result.get(\"person_metrics\", {}).get(\"index\", None)  # Extracting index\n",
    "        if not index:\n",
    "            continue\n",
    "        \n",
    "        false_positives_dict[index] = []\n",
    "        false_negatives_dict[index] = []\n",
    "        \n",
    "        for metric_key, metric_values in result.items():\n",
    "            fp_values = metric_values.get(\"false_positives\", [])\n",
    "            fn_values = metric_values.get(\"false_negatives\", [])\n",
    "            false_positives_dict[index].extend(fp_values)\n",
    "            false_negatives_dict[index].extend(fn_values)\n",
    "\n",
    "    # Write the results to the output file\n",
    "    with open(output_path, 'w') as output_file:\n",
    "        \n",
    "        # Write False Negatives\n",
    "        output_file.write(\"False Negatives:\\n\")\n",
    "        output_file.write(\"----------------------------------------\\n\")\n",
    "        for index, fn_list in false_negatives_dict.items():\n",
    "            for fn in fn_list:\n",
    "                output_file.write(fn + '\\n')\n",
    "        output_file.write(\"\\n\")\n",
    "        \n",
    "        # Write False Positives\n",
    "        output_file.write(\"False Positives:\\n\")\n",
    "        output_file.write(\"----------------------------------------\\n\")\n",
    "        for index, fp_list in false_positives_dict.items():\n",
    "            for fp in fp_list:\n",
    "                output_file.write(fp + '\\n')\n",
    "        output_file.write(\"\\n\")\n",
    "        \n",
    "        # Write by Index\n",
    "        output_file.write(\"Data by Index:\\n\")\n",
    "        output_file.write(\"----------------------------------------\\n\")\n",
    "        for index in false_positives_dict.keys():\n",
    "            output_file.write(f\"Index: {index}\\n\")\n",
    "            output_file.write(\"False Positives: \" + \", \".join(false_positives_dict[index]) + \"\\n\")\n",
    "            output_file.write(\"False Negatives: \" + \", \".join(false_negatives_dict[index]) + \"\\n\")\n",
    "            output_file.write(\"----------------------------------------\\n\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'precision_recall_updated_results.json'\n",
    "output_path = 'output_false_data.txt'\n",
    "extract_false_data(file_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
