{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c100c3-323f-4bcd-8f78-ac6abf5685ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching embedding for query: maanviljelijä\n",
      "Fetching embedding for query: käsityöt\n",
      "Fetching embedding for query: kalastus\n",
      "Fetching embedding for query: työläinen\n",
      "Fetching embedding for query: johtaja\n",
      "Fetching embedding for query: karjalaseura\n",
      "Fetching embedding for query: marttaseura\n",
      "Fetching embedding for query: palkinto\n",
      "Fetching embedding for query: menestys\n",
      "Fetching embedding for query: arvostus\n",
      "Fetching embedding for query: USA\n",
      "Fetching embedding for query: Ruotsi\n",
      "Fetching embedding for query: sotavanki\n",
      "Fetching embedding for query: menetys\n",
      "Fetching embedding for query: pettymys\n",
      "Fetching embedding for query: tuomio\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API Key from .env file and configure OpenAI client\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = api_key\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"Get embedding for a given text using OpenAI's embedding model.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=[text],\n",
    "            model=model\n",
    "        )\n",
    "        embedding = response['data'][0]['embedding']\n",
    "        return np.array(embedding)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.zeros(1024)  # Assuming the embedding size is 1024\n",
    "\n",
    "def save_embeddings(queries, output_file='keyword_embeddings.csv'):\n",
    "    \"\"\"Fetch and save embeddings for each query.\"\"\"\n",
    "    embeddings = []\n",
    "    for query in queries:\n",
    "        print(f\"Fetching embedding for query: {query}\")\n",
    "        embedding = get_embedding(query)\n",
    "        embeddings.append([query] + embedding.tolist())\n",
    "    \n",
    "    df = pd.DataFrame(embeddings, columns=['query'] + [f'embedding_{i}' for i in range(len(embeddings[0]) - 1)])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    queries = [\"maanviljelijä\", \"käsityöt\", \"kalastus\", \"työläinen\", \"johtaja\", \"karjalaseura\", \"marttaseura\", \"palkinto\", \"menestys\", \"arvostus\", \"USA\", \"Ruotsi\",\"sotavanki\",\"menetys\",\"pettymys\",\"tuomio\",\"sotavankina\"]\n",
    "    \n",
    "    # Fetch and save embeddings for each query\n",
    "    save_embeddings(queries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024161c-1ea2-4d65-bbcc-f8993414c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API Key from .env file and configure OpenAI client\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = api_key\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"Get embedding for a given text using OpenAI's embedding model.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=[text],\n",
    "            model=model\n",
    "        )\n",
    "        embedding = response['data'][0]['embedding']\n",
    "        return np.array(embedding)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return np.zeros(1024)  # Assuming the embedding size is 1024\n",
    "\n",
    "def save_embeddings(queries, output_file='keyword_embeddings.csv'):\n",
    "    \"\"\"Fetch and save embeddings for each query.\"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        df_existing = pd.read_csv(output_file)\n",
    "        existing_queries = set(df_existing['query'])\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=['query'] + [f'embedding_{i}' for i in range(1024)])\n",
    "        existing_queries = set()\n",
    "\n",
    "    new_embeddings = []\n",
    "    for query in queries:\n",
    "        if query in existing_queries:\n",
    "            print(f\"Query '{query}' already exists in the embeddings file. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Fetching embedding for query: {query}\")\n",
    "        embedding = get_embedding(query)\n",
    "        new_embeddings.append([query] + embedding.tolist())\n",
    "\n",
    "    if new_embeddings:\n",
    "        df_new = pd.DataFrame(new_embeddings, columns=['query'] + [f'embedding_{i}' for i in range(1024)])\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(output_file, index=False)\n",
    "        return df_combined\n",
    "    else:\n",
    "        return df_existing\n",
    "\n",
    "def main():\n",
    "    queries = [\"maanviljelijä\", \"käsityöt\", \"kalastus\", \"työläinen\", \"johtaja\", \"karjalaseura\", \"marttaseura\", \"palkinto\", \"menestys\", \"arvostus\", \"USA\", \"Ruotsi\",\"sotavanki\",\"menetys\",\"pettymys\",\"tuomio\",\"sotavankina\"]\n",
    "    \n",
    "    # Fetch and save embeddings for each query\n",
    "    save_embeddings(queries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072fef5b-1071-4221-b557-ae14645e1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing stories from index 0 to 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_228356/4224258118.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  existing_results = pd.concat([existing_results, new_results_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing stories from index 1000 to 2000\n",
      "Processing stories from index 2000 to 3000\n",
      "Processing stories from index 3000 to 4000\n",
      "Processing stories from index 4000 to 5000\n",
      "Processing stories from index 5000 to 6000\n",
      "Processing stories from index 6000 to 7000\n",
      "Processing stories from index 7000 to 8000\n",
      "Processing stories from index 8000 to 9000\n",
      "Processing stories from index 9000 to 10000\n",
      "Processing stories from index 10000 to 11000\n",
      "Processing stories from index 11000 to 12000\n",
      "Processing stories from index 12000 to 13000\n",
      "Processing stories from index 13000 to 14000\n",
      "Processing stories from index 14000 to 15000\n",
      "Processing stories from index 15000 to 16000\n",
      "Processing stories from index 16000 to 17000\n",
      "Processing stories from index 17000 to 18000\n",
      "Processing stories from index 18000 to 19000\n",
      "Processing stories from index 19000 to 20000\n",
      "Processing stories from index 20000 to 21000\n",
      "Processing stories from index 21000 to 22000\n",
      "Processing stories from index 22000 to 23000\n",
      "Processing stories from index 23000 to 24000\n",
      "Processing stories from index 24000 to 25000\n",
      "Processing stories from index 25000 to 26000\n",
      "Processing stories from index 26000 to 27000\n",
      "Processing stories from index 27000 to 28000\n",
      "Processing stories from index 28000 to 29000\n",
      "Processing stories from index 29000 to 30000\n",
      "Processing stories from index 30000 to 31000\n",
      "Processing stories from index 31000 to 32000\n",
      "Processing stories from index 32000 to 33000\n",
      "Processing stories from index 33000 to 34000\n",
      "Processing stories from index 34000 to 35000\n",
      "Processing stories from index 35000 to 36000\n",
      "Processing stories from index 36000 to 37000\n",
      "Processing stories from index 37000 to 38000\n",
      "Processing stories from index 38000 to 39000\n",
      "Processing stories from index 39000 to 40000\n",
      "Processing stories from index 40000 to 41000\n",
      "Processing stories from index 41000 to 42000\n",
      "Processing stories from index 42000 to 43000\n",
      "Processing stories from index 43000 to 44000\n",
      "Processing stories from index 44000 to 45000\n",
      "Processing stories from index 45000 to 46000\n",
      "Processing stories from index 46000 to 47000\n",
      "Processing stories from index 47000 to 48000\n",
      "Processing stories from index 48000 to 49000\n",
      "Processing stories from index 49000 to 50000\n",
      "Processing stories from index 50000 to 51000\n",
      "Processing stories from index 51000 to 52000\n",
      "Processing stories from index 52000 to 53000\n",
      "Processing stories from index 53000 to 54000\n",
      "Processing stories from index 54000 to 55000\n",
      "Processing stories from index 55000 to 56000\n",
      "Processing stories from index 56000 to 57000\n",
      "Processing stories from index 57000 to 58000\n",
      "Processing stories from index 58000 to 59000\n",
      "Processing stories from index 59000 to 60000\n",
      "Processing stories from index 60000 to 61000\n",
      "Processing stories from index 61000 to 62000\n",
      "Processing stories from index 62000 to 63000\n",
      "Processing stories from index 63000 to 64000\n",
      "Processing stories from index 64000 to 65000\n",
      "Processing stories from index 65000 to 66000\n",
      "Processing stories from index 66000 to 67000\n",
      "Processing stories from index 67000 to 68000\n",
      "Processing stories from index 68000 to 69000\n",
      "Processing stories from index 69000 to 70000\n",
      "Processing stories from index 70000 to 71000\n",
      "Processing stories from index 71000 to 72000\n",
      "Processing stories from index 72000 to 73000\n",
      "Processing stories from index 73000 to 74000\n",
      "Processing stories from index 74000 to 75000\n",
      "Processing stories from index 75000 to 76000\n",
      "Processing stories from index 76000 to 77000\n",
      "Processing stories from index 77000 to 78000\n",
      "Processing stories from index 78000 to 79000\n",
      "Processing stories from index 79000 to 80000\n",
      "Processing stories from index 80000 to 81000\n",
      "Processing stories from index 81000 to 82000\n",
      "Processing stories from index 82000 to 83000\n",
      "Processing stories from index 83000 to 84000\n",
      "Processing stories from index 84000 to 85000\n",
      "Processing stories from index 85000 to 86000\n",
      "Processing stories from index 86000 to 87000\n",
      "Processing stories from index 87000 to 88000\n",
      "Processing stories from index 88000 to 89000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "def load_embeddings(input_file='keyword_embeddings.csv'):\n",
    "    \"\"\"Load embeddings from the CSV file.\"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    embeddings = {row['query']: row.drop('query').values.astype(float) for _, row in df.iterrows()}\n",
    "    return embeddings\n",
    "\n",
    "def load_data_chunk(filepath, chunk_size=1000, start_idx=0):\n",
    "    \"\"\"Load a chunk of the dataset starting from the given index.\"\"\"\n",
    "    df_chunk = pd.read_csv(filepath, skiprows=range(1, start_idx + 1), nrows=chunk_size)\n",
    "    df_chunk['embedding'] = df_chunk['embedding'].apply(eval).apply(np.array)\n",
    "    return df_chunk\n",
    "\n",
    "def calculate_similarity(embeddings, story_embedding, query):\n",
    "    \"\"\"Calculate the similarity between a story embedding and a query embedding.\"\"\"\n",
    "    query_embedding = embeddings[query]\n",
    "    similarity = cosine_similarity([story_embedding], [query_embedding])[0][0] * 100  # Convert to percentage\n",
    "    return similarity\n",
    "\n",
    "def initialize_results_file(output_file, queries):\n",
    "    \"\"\"Initialize the results file with the proper columns.\"\"\"\n",
    "    columns = ['index', 'combined_text'] + [f'similarity_{query}' for query in queries]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "def calculate_similarities_chunk(df_chunk, embeddings, queries, output_file='search_results.csv'):\n",
    "    \"\"\"Calculate similarity scores for a chunk of stories with each query.\"\"\"\n",
    "    # Load existing results if available\n",
    "    if os.path.exists(output_file):\n",
    "        existing_results = pd.read_csv(output_file)\n",
    "        if 'index' not in existing_results.columns:\n",
    "            existing_results = existing_results.reset_index()\n",
    "    else:\n",
    "        initialize_results_file(output_file, queries)\n",
    "        existing_results = pd.read_csv(output_file)\n",
    "\n",
    "    if 'index' not in df_chunk.columns:\n",
    "        df_chunk = df_chunk.reset_index()\n",
    "\n",
    "    new_results = []\n",
    "\n",
    "    for i in df_chunk.index:\n",
    "        original_index = df_chunk.loc[i, 'index']\n",
    "        if original_index in existing_results['index'].values:\n",
    "            print(f\"Skipping story index: {original_index} (already calculated)\")\n",
    "            continue\n",
    "\n",
    "        story_embedding = df_chunk.loc[i, 'embedding']\n",
    "        similarities = {'index': original_index, 'combined_text': df_chunk.loc[i, 'combined_text']}\n",
    "        \n",
    "        for query in queries:\n",
    "            similarities[f'similarity_{query}'] = calculate_similarity(embeddings, story_embedding, query)\n",
    "        \n",
    "        new_results.append(pd.Series(similarities))\n",
    "\n",
    "    if new_results:\n",
    "        new_results_df = pd.DataFrame(new_results)\n",
    "        existing_results = pd.concat([existing_results, new_results_df], ignore_index=True)\n",
    "\n",
    "    # Save the updated results\n",
    "    existing_results.to_csv(output_file, index=False)\n",
    "    return existing_results\n",
    "\n",
    "def partial_similarity_calculation(filepath, embeddings, queries, start_idx=0, end_idx=None, chunk_size=1000):\n",
    "    \"\"\"Calculate similarities for stories in a specified range using chunk processing.\"\"\"\n",
    "    total_stories = sum(1 for _ in open(filepath)) - 1  # Subtract 1 for header\n",
    "    if end_idx is None or end_idx > total_stories:\n",
    "        end_idx = total_stories\n",
    "\n",
    "    for chunk_start in range(start_idx, end_idx, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, end_idx)\n",
    "        print(f\"Processing stories from index {chunk_start} to {chunk_end}\")\n",
    "        df_chunk = load_data_chunk(filepath, chunk_size, chunk_start)\n",
    "        calculate_similarities_chunk(df_chunk, embeddings, queries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\"maanviljelijä\", \"käsityöt\", \"kalastus\", \"työläinen\", \"johtaja\", \"karjalaseura\", \"marttaseura\", \"palkinto\", \"menestys\", \"arvostus\", \"USA\", \"Ruotsi\",\"sotavanki\",\"menetys\",\"pettymys\",\"tuomio\"]\n",
    "    \n",
    "    # Load previously saved embeddings\n",
    "    embeddings = load_embeddings('keyword_embeddings.csv')\n",
    "    \n",
    "    # Specify the range of stories to process\n",
    "    start_idx = 0  # Starting index\n",
    "    end_idx = 89000  # Ending index, adjust as needed\n",
    "    \n",
    "    # Calculate similarities for the specified range using chunk processing\n",
    "    partial_similarity_calculation('All_Stories_embeddings.csv', embeddings, queries, start_idx, end_idx, chunk_size=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0912ec-726a-460a-99b0-ab7eea772778",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following new queries are missing in the embeddings file: ['uusi_sana1', 'uusi_sana2']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated results with new keywords.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m missing_queries \u001b[38;5;241m=\u001b[39m [query \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m new_queries \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m embeddings]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_queries:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following new queries are missing in the embeddings file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_queries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Update results file with new keywords\u001b[39;00m\n\u001b[1;32m     59\u001b[0m updated_results \u001b[38;5;241m=\u001b[39m update_results_with_new_keywords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, embeddings, new_queries)\n",
      "\u001b[0;31mValueError\u001b[0m: The following new queries are missing in the embeddings file: ['uusi_sana1', 'uusi_sana2']"
     ]
    }
   ],
   "source": [
    "#add keywords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "def load_embeddings(input_file='keyword_embeddings.csv'):\n",
    "    \"\"\"Load embeddings from the CSV file.\"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    embeddings = {row['query']: row.drop('query').values.astype(float) for _, row in df.iterrows()}\n",
    "    return embeddings\n",
    "\n",
    "def load_data_chunk(filepath, chunk_size=1000, start_idx=0):\n",
    "    \"\"\"Load a chunk of the dataset starting from the given index.\"\"\"\n",
    "    df_chunk = pd.read_csv(filepath, skiprows=range(1, start_idx + 1), nrows=chunk_size)\n",
    "    df_chunk['embedding'] = df_chunk['embedding'].apply(eval).apply(np.array)\n",
    "    return df_chunk\n",
    "\n",
    "def calculate_similarity(embeddings, story_embedding, query):\n",
    "    \"\"\"Calculate the similarity between a story embedding and a query embedding.\"\"\"\n",
    "    query_embedding = embeddings[query]\n",
    "    similarity = cosine_similarity([story_embedding], [query_embedding])[0][0] * 100  # Convert to percentage\n",
    "    return similarity\n",
    "\n",
    "def update_results_with_new_keywords(output_file, embeddings, new_keywords):\n",
    "    \"\"\"Update the results file with new keyword similarities.\"\"\"\n",
    "    if not os.path.exists(output_file):\n",
    "        raise FileNotFoundError(f\"The results file {output_file} does not exist.\")\n",
    "    \n",
    "    existing_results = pd.read_csv(output_file)\n",
    "    \n",
    "    if 'index' not in existing_results.columns:\n",
    "        existing_results = existing_results.reset_index()\n",
    "\n",
    "    for keyword in new_keywords:\n",
    "        if f'similarity_{keyword}' in existing_results.columns:\n",
    "            print(f\"Keyword '{keyword}' already exists in the results file. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Calculating similarities for new keyword: {keyword}\")\n",
    "        existing_results[f'similarity_{keyword}'] = existing_results.apply(\n",
    "            lambda row: calculate_similarity(embeddings, eval(row['embedding']), keyword), axis=1\n",
    "        )\n",
    "\n",
    "    existing_results.to_csv(output_file, index=False)\n",
    "    return existing_results\n",
    "\n",
    "def main():\n",
    "    new_queries = [\"uusi_sana1\", \"uusi_sana2\"]  # Add new keywords here\n",
    "\n",
    "    # Load previously saved embeddings\n",
    "    embeddings = load_embeddings('keyword_embeddings.csv')\n",
    "\n",
    "    # Check if new queries exist in embeddings\n",
    "    missing_queries = [query for query in new_queries if query not in embeddings]\n",
    "    if missing_queries:\n",
    "        raise ValueError(f\"The following new queries are missing in the embeddings file: {missing_queries}\")\n",
    "\n",
    "    # Update results file with new keywords\n",
    "    updated_results = update_results_with_new_keywords('search_results.csv', embeddings, new_queries)\n",
    "    print(\"Updated results with new keywords.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98125f5-46f9-4265-8cd0-f51a3767b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to prepared_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to wrap text\n",
    "def wrap_text(text, width):\n",
    "    \"\"\"Wrap text with a given width.\"\"\"\n",
    "    return '<br>'.join([text[i:i+width] for i in range(0, len(text), width)])\n",
    "\n",
    "# Function to prepare the results dataframe\n",
    "def prepare_results_dataframe(results_df):\n",
    "    # Define the queries for each category\n",
    "    job_hobbies_queries = [\"sotavanki\"]\n",
    "    achievements_social_activities_queries = [\"tuomio\"]\n",
    "\n",
    "    # Ensure only specified similarities are used for calculations\n",
    "    results_df['job_hobbies_similarity'] = results_df.apply(\n",
    "        lambda row: np.mean([row[f'similarity_{q}'] for q in job_hobbies_queries if f'similarity_{q}' in row]), axis=1\n",
    "    )\n",
    "    results_df['achievements_similarity'] = results_df.apply(\n",
    "        lambda row: np.mean([row[f'similarity_{q}'] for q in achievements_social_activities_queries if f'similarity_{q}' in row]), axis=1\n",
    "    )\n",
    "\n",
    "    # Check for NaN values and replace them with 0\n",
    "    results_df['job_hobbies_similarity'] = results_df['job_hobbies_similarity'].fillna(0)\n",
    "    results_df['achievements_similarity'] = results_df['achievements_similarity'].fillna(0)\n",
    "\n",
    "    return results_df[['index', 'combined_text', 'job_hobbies_similarity', 'achievements_similarity']]\n",
    "\n",
    "# Load the results\n",
    "results_df = pd.read_csv('search_results.csv')\n",
    "\n",
    "# Prepare the results dataframe\n",
    "prepared_results_df = prepare_results_dataframe(results_df)\n",
    "\n",
    "# Save the prepared results to a CSV file\n",
    "output_csv_path = 'prepared_search_results.csv'\n",
    "prepared_results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a913bb3d-1d57-4b7b-9f88-2be117ad3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans clusters\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def wrap_text(text, width):\n",
    "    \"\"\"Wrap text with a given width.\"\"\"\n",
    "    return '<br>'.join([text[i:i+width] for i in range(0, len(text), width)])\n",
    "\n",
    "def visualize_results(file_path, output_dir, n_clusters=100):\n",
    "    # Load the results from the CSV file\n",
    "    results_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Verify the columns in the dataframe\n",
    "    required_columns = {'index', 'combined_text', 'job_hobbies_similarity', 'achievements_similarity'}\n",
    "    if not required_columns.issubset(results_df.columns):\n",
    "        raise ValueError(\"The CSV file does not contain the required columns.\")\n",
    "    \n",
    "    # Ensure non-negative values for the 'size' parameter\n",
    "    results_df['achievements_similarity_size'] = results_df['achievements_similarity'].apply(lambda x: max(x, 0))\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    results_df['cluster'] = kmeans.fit_predict(results_df[['job_hobbies_similarity', 'achievements_similarity']])\n",
    "\n",
    "    # Create hover text with index, wrapped story text, job hobbies similarity, and social activities similarity\n",
    "    results_df['hover_text'] = results_df.apply(\n",
    "        lambda row: (f\"<b> Index:</b> {row['index']}<br>\"\n",
    "                     f\"<b> Story:</b> {wrap_text(row['combined_text'], 80)}<br>\"\n",
    "                     f\"<b> Job/Hobbies Similarity:</b> {row['job_hobbies_similarity']:.2f}%<br>\"\n",
    "                     f\"<b> Social Activities Similarity:</b> {row['achievements_similarity']:.2f}%<br>\"\n",
    "                     f\"<b> Cluster:</b> {row['cluster']}\"), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Use the similarity scores for plotting\n",
    "    fig = px.scatter(\n",
    "        results_df,\n",
    "        x='achievements_similarity',\n",
    "        y='job_hobbies_similarity',\n",
    "        color='cluster',\n",
    "        size='achievements_similarity_size',\n",
    "        hover_data=['index', 'combined_text'],\n",
    "        title='Semantic Search Results',\n",
    "        labels={\n",
    "            'achievements_similarity': 'Achievements/Social Activities Similarity (%)',\n",
    "            'job_hobbies_similarity': 'Job/Hobbies Similarity (%)'\n",
    "        },\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Update the hovertemplate to use the custom hover text and display as a block element\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=8), \n",
    "        hovertemplate=\"<div style='white-space:normal; width:300px;'>%{customdata[0]}<extra></extra></div>\",\n",
    "        customdata=results_df[['hover_text']].values\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Semantic Search Results',\n",
    "        xaxis_title='Achievements/Social Activities Similarity (%)',\n",
    "        yaxis_title='Job/Hobbies Similarity (%)',\n",
    "        hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\", bordercolor=\"black\"),\n",
    "        xaxis=dict(range=[0, 100]),  # Ensure x-axis range is consistent\n",
    "        yaxis=dict(range=[0, 100])   # Ensure y-axis range is consistent\n",
    "    )\n",
    "\n",
    "    # Save the figure as an HTML file\n",
    "    html_file = os.path.join(output_dir, 'semantic_search_results_sotavanki_tuomio.html')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    fig.write_html(html_file)\n",
    "    \n",
    "    # Add custom JavaScript for copying to clipboard\n",
    "    with open(html_file, 'a') as f:\n",
    "        f.write(\"\"\"\n",
    "<script>\n",
    "    document.addEventListener('DOMContentLoaded', function() {\n",
    "        var plot = document.querySelector('.plotly-graph-div');\n",
    "        plot.on('plotly_click', function(data) {\n",
    "            var infotext = data.points.map(function(d) {\n",
    "                return d.customdata[0].replace(/<[^>]+>/g, '');  // Remove HTML tags for clean clipboard content\n",
    "            });\n",
    "            copyToClipboard(infotext.join('\\\\n\\\\n'));\n",
    "        });\n",
    "    });\n",
    "\n",
    "    function copyToClipboard(text) {\n",
    "        var el = document.createElement('textarea');\n",
    "        el.value = text;\n",
    "        document.body.appendChild(el);\n",
    "        el.select();\n",
    "        document.execCommand('copy');\n",
    "        document.body.removeChild(el);\n",
    "        var notification = document.createElement('div');\n",
    "        notification.innerHTML = 'Copied to clipboard';\n",
    "        notification.style.position = 'fixed';\n",
    "        notification.style.bottom = '10px';\n",
    "        notification.style.left = '10px';\n",
    "        notification.style.padding = '10px';\n",
    "        notification.style.backgroundColor = '#5cb85c';\n",
    "        notification.style.color = 'white';\n",
    "        notification.style.borderRadius = '5px';\n",
    "        document.body.appendChild(notification);\n",
    "        setTimeout(function() {\n",
    "            document.body.removeChild(notification);\n",
    "        }, 2000);\n",
    "    }\n",
    "</script>\n",
    "        \"\"\")\n",
    "\n",
    "# File path to the CSV file\n",
    "file_path = 'prepared_search_results.csv'\n",
    "\n",
    "# Directory to save the visualization\n",
    "output_dir = 'semantic_search_visualizations'\n",
    "\n",
    "# Visualize results with K-means clustering\n",
    "visualize_results(file_path, output_dir, n_clusters=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6640a3be-a0e5-44d3-a9dd-a78950ba8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stories excluded: 41599\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "def wrap_text(text, width):\n",
    "    \"\"\"Wrap text with a given width.\"\"\"\n",
    "    return '<br>'.join([text[i:i+width] for i in range(0, len(text), width)])\n",
    "\n",
    "def filter_stories_by_length(df, min_length=600):\n",
    "    \"\"\"Filter out stories with fewer than min_length characters.\"\"\"\n",
    "    original_count = len(df)\n",
    "    filtered_df = df[df['combined_text'].str.len() >= min_length]\n",
    "    excluded_count = original_count - len(filtered_df)\n",
    "    return filtered_df, excluded_count\n",
    "\n",
    "def visualize_results(file_path, output_dir):\n",
    "    # Load the results from the CSV file\n",
    "    results_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Verify the columns in the dataframe\n",
    "    required_columns = {'index', 'combined_text', 'job_hobbies_similarity', 'achievements_similarity'}\n",
    "    if not required_columns.issubset(results_df.columns):\n",
    "        raise ValueError(\"The CSV file does not contain the required columns.\")\n",
    "    \n",
    "    # Filter out stories with fewer than 200 characters\n",
    "    results_df, excluded_count = filter_stories_by_length(results_df, min_length=600)\n",
    "\n",
    "    # Ensure non-negative values for the 'size' parameter\n",
    "    results_df['achievements_similarity_size'] = results_df['achievements_similarity'].apply(lambda x: max(x, 0))\n",
    "\n",
    "    # Create hover text with index and wrapped story text\n",
    "    results_df['hover_text'] = results_df.apply(\n",
    "        lambda row: (f\"<b>Index:</b> {row['index']}<br>\"\n",
    "                     f\"<b>Story:</b> {wrap_text(row['combined_text'], 80)}<br>\"\n",
    "                     f\"<b>Job/Hobbies Similarity:</b> {row['job_hobbies_similarity']:.2f}%<br>\"\n",
    "                     f\"<b>Achievements Similarity:</b> {row['achievements_similarity']:.2f}%\"), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Use the similarity scores for plotting\n",
    "    fig = px.scatter(\n",
    "        results_df,\n",
    "        x='achievements_similarity',\n",
    "        y='job_hobbies_similarity',\n",
    "        size='achievements_similarity_size',\n",
    "        hover_data=['index', 'combined_text'],\n",
    "        title='Semantic Search Results',\n",
    "        labels={\n",
    "            'achievements_similarity': 'Achievements/Social Activities Similarity (%)',\n",
    "            'job_hobbies_similarity': 'Job/Hobbies Similarity (%)'\n",
    "        },\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Update the hovertemplate to use the custom hover text and display as a block element\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=8), \n",
    "        hovertemplate=\"<div style='white-space:normal; width:300px;'>%{customdata[0]}<extra></extra></div>\",\n",
    "        customdata=results_df[['hover_text']].values\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Semantic Search Results',\n",
    "        xaxis_title='Achievements/Social Activities Similarity (%)',\n",
    "        yaxis_title='Job/Hobbies Similarity (%)',\n",
    "        hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\", bordercolor=\"black\"),\n",
    "        xaxis=dict(range=[0, 100]),  # Ensure x-axis range is consistent\n",
    "        yaxis=dict(range=[0, 100])   # Ensure y-axis range is consistent\n",
    "    )\n",
    "\n",
    "    # Save the figure as an HTML file\n",
    "    html_file = os.path.join(output_dir, 'semantic_search_results_filtered.html')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    fig.write_html(html_file)\n",
    "    \n",
    "    # Add custom JavaScript for copying to clipboard\n",
    "    with open(html_file, 'a') as f:\n",
    "        f.write(\"\"\"\n",
    "<script>\n",
    "    document.addEventListener('DOMContentLoaded', function() {\n",
    "        var plot = document.querySelector('.plotly-graph-div');\n",
    "        plot.on('plotly_click', function(data) {\n",
    "            var infotext = data.points.map(function(d) {\n",
    "                return d.customdata[0].replace(/<[^>]+>/g, '');  // Remove HTML tags for clean clipboard content\n",
    "            });\n",
    "            copyToClipboard(infotext.join('\\\\n\\\\n'));\n",
    "        });\n",
    "    });\n",
    "\n",
    "    function copyToClipboard(text) {\n",
    "        var el = document.createElement('textarea');\n",
    "        el.value = text;\n",
    "        document.body.appendChild(el);\n",
    "        el.select();\n",
    "        document.execCommand('copy');\n",
    "        document.body.removeChild(el);\n",
    "        var notification = document.createElement('div');\n",
    "        notification.innerHTML = 'Copied to clipboard';\n",
    "        notification.style.position = 'fixed';\n",
    "        notification.style.bottom = '10px';\n",
    "        notification.style.left = '10px';\n",
    "        notification.style.padding = '10px';\n",
    "        notification.style.backgroundColor = '#5cb85c';\n",
    "        notification.style.color = 'white';\n",
    "        notification.style.borderRadius = '5px';\n",
    "        document.body.appendChild(notification);\n",
    "        setTimeout(function() {\n",
    "            document.body.removeChild(notification);\n",
    "        }, 2000);\n",
    "    }\n",
    "</script>\n",
    "        \"\"\")\n",
    "\n",
    "    print(f\"Number of stories excluded: {excluded_count}\")\n",
    "\n",
    "# File path to the CSV file\n",
    "file_path = 'prepared_search_results.csv'\n",
    "\n",
    "# Directory to save the visualization\n",
    "output_dir = 'semantic_search_visualizations'\n",
    "\n",
    "# Visualize results based on story length\n",
    "visualize_results(file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a88bee-6fb0-44fa-8832-bebaef64435c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
